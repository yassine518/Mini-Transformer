{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c63fa0b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sys.path updated: ['C:\\\\Users\\\\21629\\\\OneDrive\\\\Bureau\\\\Mini-Transformer', 'C:\\\\Python313\\\\python313.zip', 'C:\\\\Python313\\\\DLLs', 'C:\\\\Python313\\\\Lib', 'C:\\\\Python313', 'c:\\\\Users\\\\21629\\\\OneDrive\\\\Bureau\\\\Mini-Transformer\\\\.venv', '', 'c:\\\\Users\\\\21629\\\\OneDrive\\\\Bureau\\\\Mini-Transformer\\\\.venv\\\\Lib\\\\site-packages', 'c:\\\\Users\\\\21629\\\\OneDrive\\\\Bureau\\\\Mini-Transformer\\\\.venv\\\\Lib\\\\site-packages\\\\win32', 'c:\\\\Users\\\\21629\\\\OneDrive\\\\Bureau\\\\Mini-Transformer\\\\.venv\\\\Lib\\\\site-packages\\\\win32\\\\lib', 'c:\\\\Users\\\\21629\\\\OneDrive\\\\Bureau\\\\Mini-Transformer\\\\.venv\\\\Lib\\\\site-packages\\\\Pythonwin']\n"
     ]
    }
   ],
   "source": [
    "# Mini Transformer Demo Notebook\n",
    "# Fully stable training on a fixed copy-task dataset\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project root to sys.path\n",
    "project_root = os.path.abspath(os.path.join(r'C:\\Users\\21629\\OneDrive\\Bureau\\Mini-Transformer'))  # Or hardcode: r'C:\\Users\\21629\\OneDrive\\Bureau\\Mini-Transformer'\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "print(\"sys.path updated:\", sys.path)  # For debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "622f21bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from src.transformer import Transformer\n",
    "from src.masks import create_padding_mask, create_look_ahead_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6759adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\21629\\OneDrive\\Bureau\\Mini-Transformer\\.venv\\Scripts\\python.exe\n"
     ]
    }
   ],
   "source": [
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4da233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed dataset (copy task)\n",
    "def generate_fixed_dataset(num_samples, seq_len, vocab_size):\n",
    "    src = torch.randint(1, vocab_size - 1, (num_samples, seq_len))  # avoid START_TOKEN\n",
    "    tgt = src.clone()\n",
    "    return src, tgt\n",
    "\n",
    "src_train, tgt_train = generate_fixed_dataset(NUM_SAMPLES, SEQ_LEN, SRC_VOCAB_SIZE)\n",
    "src_train, tgt_train = src_train.to(DEVICE), tgt_train.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de47003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = Transformer(SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, D_MODEL, N_HEADS, D_FF,\n",
    "                    NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS).to(DEVICE)\n",
    "\n",
    "# Ensure proper weight initialization (e.g., Xavier for linear layers)\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.Embedding):\n",
    "        nn.init.normal_(m.weight, mean=0, std=0.02)\n",
    "\n",
    "model.apply(init_weights)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc47d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare decoder input (prepend start token)\n",
    "decoder_input = torch.zeros_like(tgt_train)\n",
    "decoder_input[:, 1:] = tgt_train[:, :-1]\n",
    "decoder_input[:, 0] = START_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0153d3b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/500, Loss: 2.2290, Grad Norm: 0.5119\n",
      "Epoch 100/500, Loss: 2.1233, Grad Norm: 0.3248\n",
      "Epoch 150/500, Loss: 2.0205, Grad Norm: 0.4113\n",
      "Epoch 200/500, Loss: 1.8999, Grad Norm: 0.4486\n",
      "Epoch 250/500, Loss: 1.7988, Grad Norm: 0.4568\n",
      "Epoch 300/500, Loss: 1.7114, Grad Norm: 0.4124\n",
      "Epoch 350/500, Loss: 1.6195, Grad Norm: 0.3769\n",
      "Epoch 400/500, Loss: 1.4934, Grad Norm: 0.4021\n",
      "Epoch 450/500, Loss: 1.3481, Grad Norm: 0.4804\n",
      "Epoch 500/500, Loss: 1.1859, Grad Norm: 0.4570\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    src_mask = create_padding_mask(src_train, pad_idx=PAD_IDX)\n",
    "    tgt_mask = create_padding_mask(decoder_input, pad_idx=PAD_IDX) | create_look_ahead_mask(decoder_input.size(1))\n",
    "\n",
    "    # Ensure input is long for embedding lookups\n",
    "    output = model(src_train.long(), decoder_input.long(), src_mask=src_mask, tgt_mask=tgt_mask)\n",
    "    output = output.view(-1, TGT_VOCAB_SIZE)  # reshape for loss\n",
    "    tgt_labels = tgt_train.view(-1).long()    # ensure long and reshape\n",
    "\n",
    "    loss = criterion(output, tgt_labels)\n",
    "    if torch.isnan(loss):\n",
    "        print(f\"NaN loss detected at epoch {epoch}. Stopping training.\")\n",
    "        break\n",
    "    loss.backward()\n",
    "\n",
    "    # Gradient clipping\n",
    "    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 50 == 0:\n",
    "        print(f\"Epoch {epoch}/{EPOCHS}, Loss: {loss.item():.4f}, Grad Norm: {grad_norm:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df592587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished and model saved ✅\n"
     ]
    }
   ],
   "source": [
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"mini_transformer.pth\")\n",
    "print(\"Training finished and model saved ✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9b1325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a sequence from fixed dataset\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    src_test = src_train[:1]\n",
    "    tgt_test = tgt_train[:1]\n",
    "\n",
    "    decoder_input_test = torch.zeros_like(tgt_test)\n",
    "    decoder_input_test[:, 1:] = tgt_test[:, :-1]\n",
    "    decoder_input_test[:, 0] = START_TOKEN\n",
    "\n",
    "    src_mask = create_padding_mask(src_test, pad_idx=PAD_IDX)\n",
    "    tgt_mask = create_padding_mask(decoder_input_test, pad_idx=PAD_IDX) | create_look_ahead_mask(decoder_input_test.size(1))\n",
    "\n",
    "    logits = model(src_test.long(), decoder_input_test.long(), src_mask=src_mask, tgt_mask=tgt_mask)\n",
    "    pred_tokens = logits.argmax(dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70044c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequence:      [[3 5 7 4 4]]\n",
      "Target sequence:     [[3 5 7 4 4]]\n",
      "Predicted sequence:  [[3 5 4 4 4]]\n"
     ]
    }
   ],
   "source": [
    "# Show results\n",
    "print(\"Input sequence:     \", src_test.cpu().numpy())\n",
    "print(\"Target sequence:    \", tgt_test.cpu().numpy())\n",
    "print(\"Predicted sequence: \", pred_tokens.cpu().numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
